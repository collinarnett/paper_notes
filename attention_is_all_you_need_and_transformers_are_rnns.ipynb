{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes: Attention Is All You Need and Transformers are RNNs\n",
    "\n",
    "1. Attention Is All You Need\n",
    "2. Transformers are RNNs:Fast Autoregressive Transformers with Linear Attention\n",
    "\n",
    "## Introduction to Transformers\n",
    "\n",
    "Attention Is All You Need (AIAYN) introduces the concept of a Transformer. They outline the transformer in the following figure:\n",
    "\n",
    "![transformer diagram](https://i.imgur.com/thw3UDN.png)\n",
    "\n",
    "The transformer consists of a encoder and decoder. The goal of the transformer is to encode a source set of tokens into a hidden state to be passed to the multi-head attention (more on this in a sec) portion of the decoder in order to predict the next token in a sequence. A simple application example would be using a transformer to translate one sentence to another.\n",
    "\n",
    "### Attention\n",
    "\n",
    "The meat of the AIAYN paper is in their use of what they deem Attention. Their attention mechanism is defined as:\n",
    "\n",
    "$$\\mathrm{Attention} (Q,K,V)=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})\\mathrm{V}$$\n",
    "\n",
    "* $Q = \\mathrm{Querys}$ \n",
    "* $K = \\mathrm{Keys}$ \n",
    "* $V = \\mathrm{Values}$ \n",
    "\n",
    "where they also list the following diagrams:\n",
    "\n",
    "![attention mechanism](https://i.imgur.com/rMHl7xy.png)\n",
    "\n",
    "\n",
    "## Transformers are RNNs\n",
    "\n",
    "This paper outlines the problem of transformers where computing the Scaled Dot Product Attention results in a quadratic complexity. The authors propose using a kernel to downgrade the dimensionality of the attention equation.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\\mathrm{V'} =\\mathrm{Attention} (Q,K,V)=\\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})\\mathrm{V}$$\n",
    "\n",
    "we can rewrite as:\n",
    "\n",
    "$$\\mathrm{V'_i} = \\frac{\\sum^N_{j=1} \\mathrm{sim}(Q_i,K_j)V_j}{\\sum^N_{j=1} \\mathrm{sim}(Q_i,K_j)}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\\mathrm{sim}(q,k) = \\mathrm{exp}(\\frac{q^T k}{\\sqrt{D}})$$\n",
    "\n",
    "### Linearization \n",
    "\n",
    "From here the authors introduce their method for linearized attention where $\\phi(x)$ represents their kernel function and simplify the above equation:\n",
    "\n",
    "$$\\mathrm{V'_i} = \\frac{\\sum^N_{j=1} \\phi(Q_i)^T \\phi(K_j)V_j}{\\sum^N_{j=1} \\phi(Q_i)^T \\phi(K_j)}$$\n",
    "\n",
    "$$\\mathrm{V'_i} = \\frac{ \\phi(Q_i)^T \\sum^N_{j=1}  \\phi(K_j)V_j}{\\phi(Q_i)^T \\sum^N_{j=1} \\phi(K_j)}$$\n",
    "\n",
    "They explain that this results in better performance in the following quote:\n",
    "\n",
    "![complexity_simplification](https://i.imgur.com/0jrSzvd.png)\n",
    "\n",
    "Where equation 2 represents the original scaled dot product attention.\n",
    "\n",
    "### The connection between RNNs and Transformers\n",
    "\n",
    "The authors make the connection between RNNs and Transformers by first defining how they implement causal masking. This makes training the the transformer really fast since you can paralleize for each token in a given sequence. They do this by restricting each token so it can only look at the token in the input sequence at the same position or previous position like so:\n",
    "\n",
    "$\\mathrm{V'_i} = \\frac{ \\phi(Q_i)^T \\sum^N_{j=1}  \\phi(K_j)V_j}{\\phi(Q_i)^T \\sum^N_{j=1} \\phi(K_j)}$ becomes $\\mathrm{V'_i} = \\frac{ \\phi(Q_i)^T \\sum^i_{j=1}  \\phi(K_j)V^T_j}{\\phi(Q_i)^T \\sum^i_{j=1} \\phi(K_j)}$\n",
    "\n",
    "this + the linearization means that you can calculate $\\phi(K_j)V^T_j$ once per time step and just store it.\n",
    "\n",
    "They then simplify to:\n",
    "\n",
    "$$S_i=\\sum^i_{j=1}\\phi(K_i)(V^T_j)$$\n",
    "\n",
    "$$Z_i=\\sum^i_{j=1}\\phi(K_i)$$\n",
    "\n",
    "which allows for the authors to interpret transformers as RNNs as follows:\n",
    "\n",
    "$s_0=0$\n",
    "\n",
    "$z_0=0$\n",
    "\n",
    "$s_i=s_{i-1} +\\phi(x_iW_K)(x_iW_V)^T$\n",
    "\n",
    "$z_i=z_{i-1} +\\phi(x_iW_K)$\n",
    "\n",
    "$y_i= f_l(\\frac{x_iW_Q)^Ts_i}{x_iW_Q)^Tz_i}+x_i)$\n",
    "\n",
    "### Performance\n",
    "\n",
    "They get significantly better performance in the linear transformer's inference time as well as the training time (as expected). They also use significantly less memory due to fact they only have to calculate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
