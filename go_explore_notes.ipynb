{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First return then explore (Go-Explore) - Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High Level Overview\n",
    "\n",
    "Problems this paper aims to solve:\n",
    "* sparse rewards\n",
    "  * detachment - The agent fails to find interesting places to explore from (eg. stuck in one room of a house)\n",
    "  * derailment - The agent fails to find previous states\n",
    "\n",
    "#### Cells and States\n",
    "\n",
    "Go-Explore solves the above problems by archiving states in *cells* so the algorithm can return to interesting states it's been to before to further explore from there, hence solving the detachment and derailment problem.\n",
    "\n",
    "![cell_state](https://i.imgur.com/ygnEQZ1.png)\n",
    "\n",
    "![exploration_phase](https://i.imgur.com/3AVzFhb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author's note that one benefit of saving state cells in a enviornment like Atari benchmark suite is that you don't need a policy to return to states. You can just pick a previous cell and reset the simulator to that state and take random actions to explore from there. This is nice if you have access to the underlying infra of the simulator but is not very useful when you don't have a lot of control over the enviornment eg most closed source games. For this reason they also introduce a policy for this scenario later in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Based Go-Explore\n",
    "\n",
    "![image](https://i.imgur.com/OkHCJq5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors select the next cell based on the following metric where $C_{steps}$ is the number of steps the agent spent in the cell.\n",
    "$$W= \\frac{1}{0.5C_{steps}+1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robustification\n",
    "\n",
    "Robustification is basically taking a trajectory (series of steps) and doing imitation learning. Eg if you start at the end of the trajectory you walk back a few steps and have the agent learn to reach the goal and repeat this process until your agent is able to reach the goal. This helps the agent learn in stochastic enviornments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "\"In terms of infrastructure, each exploration phase run was performed on a single worker machine\n",
    "equipped with 44 CPUs and 96GB of RAM, though memory usage is substantially lower for most games. Each\n",
    "robustification run was parallelized across 8 worker machines each equipped with 11 CPUs, 24GB of RAM, and 1\n",
    "GPU. Policy-based Go-Explore was parallelized across 16 worker machines each equipped with 11 CPUs, 10GB of\n",
    "RAM, and 1 GPU.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
